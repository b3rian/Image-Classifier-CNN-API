# Vision Transformer (ViT) configuration for training

dataset:
  name: ImageNet-200
  train_augmentation:
    resize_shorter_side: 256
    crop_size: 224
    random_horizontal_flip: true
    normalize: true
  test_augmentation:
    center_crop: true
    resize_shorter_side: 256
    crop_size: 224

model:
  name: VisionTransformer
  input_shape: [64, 64, 3]
  num_classes: 200
  patch_size: 16
  hidden_dim: 768
  num_heads: 12
  num_layers: 12
  mlp_dim: 3072
  dropout: 0.1
  attention_dropout: 0.0
  classifier: token
  pretrained: false # train from scratch

training:
  optimizer: Adam
  adam_betas: [0.9, 0.999]
  batch_size: 4096
  learning_rate:
    warmup: linear
    schedule: linear_decay
  weight_decay: 0.1
  max_epochs: 100 # Total number of epochs for training
  gradient_clipping: null
  label_smoothing: 0.1